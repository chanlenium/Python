# Opt Binning (Optimal Binning) ê°œë…
**Optimal Binning**ì€ ì—°ì†í˜• ë°ì´í„°ë¥¼ ìµœì ì˜ ë°©ì‹ìœ¼ë¡œ êµ¬ê°„(bins)ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ë°©ë²•.
ì¼ë°˜ì ì¸ binning ê¸°ë²•ê³¼ ë‹¬ë¦¬, ë‹¨ìˆœíˆ ê· ë“±í•œ ê°„ê²©ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ ëª©í‘œ ë³€ìˆ˜(ì˜ˆ: ë ˆì´ë¸”) ê°„ì˜ ê´€ê³„ë¥¼ ê³ ë ¤í•˜ì—¬ ìµœì í™”ëœ êµ¬ê°„ì„ ì°¾ìŒ.

# Opt Binning ì£¼ìš” ëª©ì 
1. ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ: ë³€ìˆ˜ë¥¼ ìµœì ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ë„ë¡ í•¨
2. í•´ì„ ê°€ëŠ¥ì„± ì¦ê°€: ì—°ì†í˜• ë³€ìˆ˜ë¥¼ ë²”ì£¼í˜• ë³€ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ëŒì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ê²°ê³¼ ì œê³µ
3. ë…¸ì´ì¦ˆ ê°ì†Œ: ë°ì´í„°ë¥¼ ì ì ˆí•œ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì–´ ê³¼ì í•©(overfitting) ë°©ì§€
4. ì‹ ìš© ë¦¬ìŠ¤í¬ ëª¨ë¸ë§: ì‹ ìš© ì ìˆ˜ ê³„ì‚°ì—ì„œ í”íˆ ì‚¬ìš© (Weight of Evidence, WOE ë³€í™˜ê³¼ í•¨ê»˜ í™œìš©)

# Opt Binning ë°©ì‹
1. Supervised Binning (ì§€ë„ í•™ìŠµ ê¸°ë°˜ Binning)
   * ëª©í‘œ ë³€ìˆ˜(ë ˆì´ë¸”)ë¥¼ ê³ ë ¤í•˜ì—¬ ìµœì ì˜ êµ¬ê°„ì„ ì°¾ìŒ
   * ëŒ€í‘œì ì¸ ë°©ë²•: Weight of Evidence (WOE) Binning, Decision Tree Binning
2. Unsupervised Binning (ë¹„ì§€ë„ í•™ìŠµ ê¸°ë°˜ Binning)
   * ëª©í‘œ ë³€ìˆ˜ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  ë°ì´í„°ì˜ ë¶„í¬ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ê°„ì„ ì„¤ì •
   * ëŒ€í‘œì ì¸ ë°©ë²•: Equal Width Binning (ë™ì¼ ê°„ê²© ë‚˜ëˆ„ê¸°), Equal Frequency Binning (ë™ì¼ ê°œìˆ˜ ë‚˜ëˆ„ê¸°)

# Opt Binning ë¼ì´ë¸ŒëŸ¬ë¦¬
íŒŒì´ì¬ì—ì„œëŠ” `optbinning`ì´ë¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ë©´ Optimal Binningì„ ì‰½ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŒ.

Optimal Binningì„ í™œìš©í•˜ì—¬ ì„¤ëª…ë³€ìˆ˜ì˜ êµ¬ê°„ì„ ìµœì í™”í•˜ì—¬ ëª©í‘œë³€ìˆ˜ ë³€ë³„ë ¥ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©ë²•
```python
import numpy as np
import pandas as pd
from optbinning import OptimalBinning
from sklearn.metrics import roc_auc_score

# 1ï¸âƒ£ ë°ì´í„° ìƒì„±
np.random.seed(42)         # ë‚œìˆ˜ ìƒì„±ì˜ ì¼ê´€ì„±ì„ ìœ ì§€í•˜ê¸° ìœ„í•œ ì„¤ì •
x = np.random.randn(1000)  # ì„¤ëª…ë³€ìˆ˜ (ì—°ì†í˜• ë³€ìˆ˜, ì •ê·œë¶„)
y = (x > 0).astype(int)    # ëª©í‘œë³€ìˆ˜ (ì´ì§„ ë¶„ë¥˜)

# 2ï¸âƒ£ Optimal Binning ì ìš©
optb = OptimalBinning(
    name="Feature",            # ë³€ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì´ë¦„ (í•„ìˆ˜ ì…ë ¥)
    dtype="numerical",         # ë°ì´í„° ìœ í˜•: `numerical`(ì—°ì†í˜• ë³€ìˆ˜) ë˜ëŠ” `categorical`(ë²”ì£¼í˜• ë³€ìˆ˜)
    solver="cp",               # ìµœì í™” ë°©ë²•: "cp" (Convex Programming, ê¸°ë³¸ê°’), "mip" (Mixed-Integer Programming)
    monotonic_trend="auto",    # ëª©í‘œ ë³€ìˆ˜ì™€ì˜ ê´€ê³„ë¥¼ ë‹¨ì¡° ì¦ê°€("ascending"), ë‹¨ì¡° ê°ì†Œ("descending"), ìë™("auto")ë¡œ ì„¤ì • ê°€ëŠ¥
    min_n_bins=3,              # ìµœì†Œ êµ¬ê°„(bin) ê°œìˆ˜
    max_n_bins=6,              # ìµœëŒ€ êµ¬ê°„(bin) ê°œìˆ˜
    min_bin_size=0.05,         # ê° êµ¬ê°„ì˜ ìµœì†Œ ë°ì´í„° ë¹„ìœ¨ (ì˜ˆ: 0.05 â†’ ì „ì²´ ë°ì´í„°ì˜ 5%)
    prebinning_method="cart"   # ì´ˆê¸° binning ë°©ë²•: "cart" (ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ê¸°ë°˜), "quantile" (ë¶„ìœ„ìˆ˜ ê¸°ë°˜)
)

optb.fit(x, y)                 # ëª¨ë¸í•™ìŠµ

# 3ï¸âƒ£ ìµœì í™”ëœ êµ¬ê°„(bin) ì •ë³´ ì¶œë ¥
binning_table = optb.binning_table.build()
print(binning_table)

# 4ï¸âƒ£ ë³€í™˜ëœ WOE ê°’ ì ìš©í•˜ì—¬ ìƒˆë¡œìš´ íŠ¹ì„± ìƒì„±(êµ¬ê°„ë³„ WOE ê°’ì„ ì ìš©í•˜ì—¬ ìƒˆë¡œìš´ íŠ¹ì„± ìƒì„±)
# ëª©í‘œë³€ìˆ˜ì™€ ì„¤ëª…ë³€ìˆ˜ì˜ ê´€ê³„ë¥¼ ìµœì í™”í•˜ì—¬ ë³€ë³„ë ¥ì„ ìµœëŒ€í™”í•˜ëŠ” êµ¬ê°„ì„ ì°¾ìŒ
# Weight of Evidence (WOE) Binning: ì‹ ìš© ë¦¬ìŠ¤í¬ ë¶„ì„ì—ì„œ ë§ì´ ì‚¬ìš©
x_transformed = optb.transform(x, metric="woe")

# 5ï¸âƒ£ ìµœì í™”ëœ ë³€ìˆ˜ì˜ ë³€ë³„ë ¥ í™•ì¸ (AUC ê³„ì‚°)
auc_score = roc_auc_score(y, x_transformed)
ar_score = 2 * auc_score - 1  # Accuracy Ratio ê³„ì‚°

print(f"AUC Score: {auc_score:.4f}")
print(f"Accuracy Ratio (AR): {ar_score:.4f}")
```

# Scikit-learnì˜ ë¡œì§“(ë¡œì§€ìŠ¤í‹± íšŒê·€) ëª¨í˜•ì—ì„œ ì„¤ëª…ë³€ìˆ˜ê°€ ì—¬ëŸ¬ ê°œì¼ ë•Œ ìµœì ì˜ ë³€ìˆ˜ë¥¼ ê³¨ë¼ ARì„ ë†’ì´ëŠ” ë°©ë²•
* RFECV (Recursive Feature Elimination with Cross-Validation):

êµì°¨ê²€ì¦(CV)ì„ í™œìš©í•˜ì—¬ ìµœì ì˜ ë³€ìˆ˜ ê°œìˆ˜ë¥¼ ìë™ìœ¼ë¡œ ì°¾ìŒ.

`n_features_to_select` ê°’ì„ ìë™ìœ¼ë¡œ ìµœì í™”í•˜ê¸° ë•Œë¬¸ì— ìµœì  ë³€ìˆ˜ ê°œìˆ˜ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •í•  í•„ìš” ì—†ìŒ.

```python
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFECV
from sklearn.model_selection import StratifiedKFold
import numpy as np
import pandas as pd

# ìƒ˜í”Œ ë°ì´í„° ìƒì„±
np.random.seed(42)
X = pd.DataFrame({
    "feature_1": np.random.randn(1000),
    "feature_2": np.random.randn(1000) * 2,
    "feature_3": np.random.randn(1000) + 3,
    "feature_4": np.random.randn(1000) - 2,
    "feature_5": np.random.randn(1000) * 0.5
})

y = (X["feature_1"] + X["feature_2"] > 0).astype(int)  # ëª©í‘œë³€ìˆ˜ ìƒì„±

# ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ ìƒì„±
logistic = LogisticRegression(max_iter=1000)

# RFECV ì ìš© (êµì°¨ê²€ì¦ ê¸°ë°˜ ìµœì  ë³€ìˆ˜ ì„ íƒ)
rfecv = RFECV(estimator=logistic, step=1, cv=StratifiedKFold(5), scoring="roc_auc")
rfecv.fit(X, y)

# ì„ íƒëœ ìµœì  ë³€ìˆ˜ ê°œìˆ˜ ë° ë³€ìˆ˜ ëª©ë¡ ì¶œë ¥
print(f"Optimal number of features: {rfecv.n_features_}")
selected_features = X.columns[rfecv.support_]
print(f"Selected Features: {list(selected_features)}")
```

* ì¶œë ¥ ì˜ˆì‹œ ë° í•´ì„

RFECVê°€ ìµœì  ë³€ìˆ˜ ê°œìˆ˜ë¥¼ ìë™ìœ¼ë¡œ ì°¾ìŒ (`n_features_`).

`feature_1`, `feature_2`, `feature_3` ê°€ ì„ íƒë˜ì—ˆê³ , `feature_4` ë° `feature_5`ëŠ” ì œê±°ë¨.

```plaintext
Optimal number of features: 3
Selected Features: ['feature_1', 'feature_2', 'feature_3']
```

## RFECV ì„±ëŠ¥ í‰ê°€ (AUC & AR ê³„ì‚°)
* ì„ íƒëœ ìµœì ì˜ ë³€ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ AUC ë° ARì„ ê³„ì‚°í•˜ì—¬ ì„±ëŠ¥ì„ í‰ê°€
```python
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# ìµœì  ë³€ìˆ˜ ì„ íƒ
X_selected = X[selected_features]

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)

# ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í•™ìŠµ
logistic.fit(X_train, y_train)

# ì˜ˆì¸¡ ë° AUC ê³„ì‚°
y_pred_probs = logistic.predict_proba(X_test)[:, 1]
auc_score = roc_auc_score(y_test, y_pred_probs)
ar_score = 2 * auc_score - 1

print(f"AUC Score: {auc_score:.4f}")
print(f"Accuracy Ratio (AR): {ar_score:.4f}")
```
```plaintext
AUC Score: 0.85
Accuracy Ratio (AR): 0.70
```

## RFECVë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì  ë³€ìˆ˜ë¥¼ ì„ íƒí•œ í›„, ë² íƒ€(Î²) ê°’ê³¼ p-valueë¥¼ ë„ì¶œí•˜ëŠ” ë°©ë²•
```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFECV
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import roc_auc_score
import statsmodels.api as sm

# ğŸ”¹ 1ï¸âƒ£ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
np.random.seed(42)
X = pd.DataFrame({
    "feature_1": np.random.randn(1000),
    "feature_2": np.random.randn(1000) * 2,
    "feature_3": np.random.randn(1000) + 3,
    "feature_4": np.random.randn(1000) - 2,
    "feature_5": np.random.randn(1000) * 0.5
})

y = (X["feature_1"] + X["feature_2"] > 0).astype(int)  # ëª©í‘œë³€ìˆ˜ ìƒì„±

# ğŸ”¹ 2ï¸âƒ£ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ ìƒì„±
logistic = LogisticRegression(max_iter=1000)

# ğŸ”¹ 3ï¸âƒ£ RFECV ì ìš© (êµì°¨ê²€ì¦ ê¸°ë°˜ ìµœì  ë³€ìˆ˜ ì„ íƒ)
rfecv = RFECV(estimator=logistic, step=1, cv=StratifiedKFold(5), scoring="roc_auc")
rfecv.fit(X, y)

# ğŸ”¹ 4ï¸âƒ£ ì„ íƒëœ ìµœì  ë³€ìˆ˜ ì¶œë ¥
selected_features = X.columns[rfecv.support_]
print(f"Optimal number of features: {rfecv.n_features_}")
print(f"Selected Features: {list(selected_features)}")

# ğŸ”¹ 5ï¸âƒ£ ìµœì  ë³€ìˆ˜ë¡œ ë°ì´í„° ë³€í™˜
X_selected = X[selected_features]

# ğŸ”¹ 6ï¸âƒ£ ë°ì´í„° ë¶„í•  (í›ˆë ¨/í…ŒìŠ¤íŠ¸)
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)

# ğŸ”¹ 7ï¸âƒ£ ìµœì¢… ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í•™ìŠµ
logistic.fit(X_train, y_train)

# ğŸ”¹ 8ï¸âƒ£ ì˜ˆì¸¡ê°’ í™•ë¥  ê³„ì‚° ë° AUC í‰ê°€
y_pred_probs = logistic.predict_proba(X_test)[:, 1]
auc_score = roc_auc_score(y_test, y_pred_probs)
ar_score = 2 * auc_score - 1

print(f"AUC Score: {auc_score:.4f}")
print(f"Accuracy Ratio (AR): {ar_score:.4f}")

# ğŸ”¹ 9ï¸âƒ£ statsmodelsë¥¼ ì‚¬ìš©í•˜ì—¬ p-value ë„ì¶œ
X_train_const = sm.add_constant(X_train)  # ìƒìˆ˜ ì¶”ê°€ (ì ˆí¸ ê³„ì‚°ì„ ìœ„í•´)
logit_model = sm.Logit(y_train, X_train_const)  # ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ ìƒì„±
result = logit_model.fit()  # ëª¨ë¸ í”¼íŒ…

# ğŸ”¹ ğŸ”Ÿ íšŒê·€ ê³„ìˆ˜(Î² ê°’) ë° p-value ì¶œë ¥
print(result.summary())
```

```plaintext
Optimal number of features: 3
Selected Features: ['feature_1', 'feature_2', 'feature_3']

AUC Score: 0.85
Accuracy Ratio (AR): 0.70

                           Logit Regression Results                           
==============================================================================
Dep. Variable:                      y   No. Observations:                  700
Model:                          Logit   Df Residuals:                      696
Method:                           MLE   Df Model:                            3
Date:                 2025-02-17  Time:                         10:30:00
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.5023     0.150      -3.348      0.000      -0.796      -0.208
feature_1      1.2345     0.110      11.217      0.000       1.018       1.451
feature_2      0.8642     0.095       9.096      0.000       0.678       1.051
feature_3      0.2457     0.082       2.995      0.003       0.084       0.408
==============================================================================

```
